{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear and Logistic Regression in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression in Tensorflow\n",
    "-----\n",
    "    Dataset Description:\n",
    "    Name: Fire and Theft in Chicago\n",
    "    X = fires per 1000 housing units\n",
    "    Y = thefts per 1000 population\n",
    "\n",
    "    within the same Zip code in the Chicago metro area\n",
    "    Total number of Zip code areas: 42\n",
    "    \n",
    "    Solution:\n",
    "    First, assume that the relationship between the number of fires and thefts are linear:\n",
    "    Y = wX + b\n",
    "\n",
    "    another try:\n",
    "    Y = wXX + uX + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=1.7183813 b=15.789157\n",
      "loss=25.375277\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcd77cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import xlrd\n",
    "\n",
    "DATA_FILE = \"data/slr05.xls\"\n",
    "\n",
    "# Step 1: read in data from the .xls file\n",
    "book = xlrd.open_workbook(DATA_FILE, encoding_override=\"utf-8\")\n",
    "sheet = book.sheet_by_index(0)\n",
    "data = np.asarray([sheet.row_values(i) for i in range(1, sheet.nrows)])\n",
    "n_samples = sheet.nrows - 1\n",
    "\n",
    "# Step 2: create placeholders for input X (number of fire) and label Y (number of theft)\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "\n",
    "# Step 3: create weight and bias, initialized to 0\n",
    "w = tf.Variable(0.0, name=\"weights\")\n",
    "# w_2 = tf.Variable(0.0, name=\"weights 2\")\n",
    "b = tf.Variable(0.0, name=\"bias\")\n",
    "\n",
    "# Step 4: construct model to predict Y (number of theft) from the number of fire\n",
    "Y_predicted = X * w + b\n",
    "#Y_predicted = X * X * w + X * u + b\n",
    "\n",
    "# Step 5: use the square error as the loss function\n",
    "loss = tf.square(Y - Y_predicted, name=\"loss\")\n",
    "\n",
    "# Step 6: using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "x_r = []\n",
    "y_r = []\n",
    "with tf.Session() as sess:\n",
    "    # Step 7: initialize the necessary variables, in this case, w and b\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Step 8: train the model\n",
    "    for i in range(100): # run 100 epochs\n",
    "        for x, y in data:\n",
    "            # Session runs train_op to minimize loss\n",
    "            sess.run(optimizer, feed_dict={X: x, Y:y})\n",
    "    \n",
    "    # Step 9: output the values of w and b\n",
    "    w_value, b_value = sess.run([w, b])\n",
    "    print(\"w=%r b=%r\" % (w_value, b_value))\n",
    "    for x, y in data:\n",
    "        x_r.append(x)\n",
    "        y_r.append(sess.run(Y_predicted, feed_dict={X: x, Y:y}))\n",
    "    \n",
    "    # plot\n",
    "    loss_r = sess.run(loss, feed_dict={X: x, Y:y})\n",
    "    print(\"loss=%r\" % loss_r)\n",
    "    \n",
    "    plt.figure('linear regression')\n",
    "    plt.plot(data[:,0], data[:,1], '.', label=\"Real Data\")\n",
    "    plt.plot(x_r, y_r, '.', label=\"Predicted data\")\n",
    "    plt.legend(loc='upper right', frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimizer\n",
    "---------------\n",
    "        GradientDescentOptimizer means that our update rule is gradient descent. TensorFlow does auto differentiation for us, then update the values of w and b to minimize the loss.\n",
    "        By default, the optimizer trains all the trainable variables whose objective function depend on. If there are variables that you do not want to train, you can set the keyword trainable to False when you declare a variable. One example of a variable you don’t want to train is the variable global_step, a common variable you will see in many TensorFlow model to keep track of how many times you’ve run your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "global_step = tf.Variable(0, trainable=False, dtype=tf.int32) # global step counter\n",
    "learning_rate = 0.01 * 0.99 ** tf.cast(global_step, tf.float32) # convert from int32 to float32\n",
    "increment_step = global_step.assign_add(1) # ???\n",
    "optimizer = tf.GradientDescentOptimizer(learning_rate) # learning rate can be a tensor\n",
    "train_op = optimizer.minimize(loss, global_step=increment_step)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also ask your optimizer to take gradients of specific variables. You can also modify the\n",
    "gradients calculated by your optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# create an optimizer.\n",
    "optimizer = GradientDescentOptimizer(learning_rate=0.1)\n",
    "# compute the gradients for a list of variables.\n",
    "grads_and_vars = opt.compute_gradients(loss, <list of variables>)\n",
    "# grads_and_vars is a list of tuples (gradient, variable). Do whatever you\n",
    "# need to the 'gradient' part, for example, subtract each of them by 1.\n",
    "subtracted_grads_and_vars = [(gv[0] - 1.0, gv[1]) for gv in grads_and_vars]\n",
    "# ask the optimizer to apply the subtracted gradients.\n",
    "optimizer.apply_gradients(subtracted_grads_and_vars)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression in TensorFlow\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "n_batches=429 test_num=55000 loss=0.40479055\n",
      "total_correct_preds=50009.0 Accuracy=5.0008999999999997\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "MNIST = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "# Step 2: Define parameters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 25\n",
    "\n",
    "# Step 3: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9.\n",
    "# each label is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10])\n",
    "\n",
    "# Step 4: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name=\"weights\")\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 5: predict Y from X and w, b\n",
    "# the model that returns probability distribution of possible label of the image\n",
    "# through the softmax layer\n",
    "# a batch_size x 10 tensor that represents the possibility of the digits\n",
    "logits = tf.matmul(X, w) + b\n",
    "\n",
    "# Step 6: define loss function\n",
    "# use softmax cross entropy with logits as the loss function\n",
    "# compute mean cross entropy, softmax is applied internally\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over examples in the batch\n",
    "\n",
    "# Step 7: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize cost\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('./graphs/logistic_reg', sess.graph)\n",
    "    \n",
    "    sess.run(init)\n",
    "    n_batches = int(MNIST.train.num_examples/batch_size)\n",
    "    for i in range(n_epochs): # train the model n_epochs times\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = MNIST.train.next_batch(batch_size)\n",
    "            _, loss_r = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch})\n",
    "\n",
    "    # average loss should be around 0.35 after 25 epochs\n",
    "    print(\"n_batches=%r test_num=%r loss=%r\" % (n_batches, MNIST.train.num_examples, loss_r))\n",
    "    \n",
    "    # test the model\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = MNIST.test.next_batch(batch_size)\n",
    "        preds = tf.nn.softmax(logits)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch})\n",
    "        #print(\"total_correct_preds=%r\" % (total_correct_preds))\n",
    "    print(\"total_correct_preds=%r Accuracy=%r\" % (total_correct_preds, total_correct_preds/MNIST.test.num_examples))\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
